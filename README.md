# 3D-Reconstruction
**Laparoscopic stereo matching using 3-Dimensional Fourier transform with full multi-scale features**
  
  Abstract:3-Dimensional (3D) reconstruction of laparoscopic surgical scenes is a key task for future surgical navigation and automated robotic minimally invasive surgery. Binocular laparoscopy with stereo matching enables 3D reconstruction. Stereo matching models used for natural images such as autopilot tend to be less suitable for laparoscopic environments due to the constraints of small samples of laparoscopic images, complex textures, and uneven illumination. In addition, current stereo matching modules use 3D convolutions and transformers in the spatial domain as the base module, which is limited by the ability to learn in the spatial domain. In this paper, we propose a model for laparoscopic stereo matching using 3D Fourier Transform combined with Full Multi-scale Features (FT-FMF Net). Specifically, the proposed Full Multi-scale Fusion Module (FMFM) is able to fuse the full multi-scale feature information from the feature extractor into the stereo matching block, which densely learns the feature information with parallax and FMFM fusion information in the frequency domain using the proposed Dense Fourier Transform Module (DFTM). We validated the proposed method in both the laparoscopic dataset (SCARED) and the endoscopic dataset (SERV-CT). In comparison with other popular and advanced deep learning models available at present, FT-FMF Net achieves the most advanced stereo matching performance available. In the SCARED and SERV-CT public datasets, the End-Point-Error (EPE) was 0.7265 and 2.3119, and the Root Mean Square Error Depth (RMSE Depth) was 4.00 mm and 3.69 mm, respectively. In addition, the inference time is only 0.17s. Our project code is available on https://github.com/wurenkai/FT-FMF.



**Self-supervised Monocular Depth and Pose Estimation for Endoscopy with Generative Latent Priors**
https://arxiv.org/pdf/2411.17790
Accurate 3D mapping in endoscopy enables quantitative, holistic lesion characterization within the gastrointestinal (GI) tract, requiring reliable depth and pose estimation. However, endoscopy systems are monocular, and existing methods relying on synthetic datasets or complex models often lack generalizability in challenging endoscopic conditions. We propose a robust self-supervised monocular depth and pose estimation framework that incorporates a Generative Latent Bank and a Variational Autoencoder (VAE). The Generative Latent Bank leverages extensive depth scenes from natural images to condition the depth network, enhancing realism and robustness of depth predictions through latent feature priors. For pose estimation, we reformulate it within a VAE framework, treating pose transitions as latent variables to regularize scale, stabilize z-axis prominence, and improve x-y sensitivity. This dual refinement pipeline enables accurate depth and pose predictions, effectively addressing the GI tract’s complex textures and lighting. Extensive evaluations on SimCol and EndoSLAM datasets confirm our framework’s superior performance over published self-supervised methods in endoscopic depth and pose estimation.

**PR-ENDO: Physically Based Relightable Gaussian Splatting for Endoscopy**
https://arxiv.org/pdf/2411.12510
Endoscopic procedures are crucial for colorectal cancer diagnosis, and three-dimensional reconstruction of the environment for real-time novel-view synthesis can significantly enhance diagnosis. We present PR-ENDO, a framework that leverages 3D Gaussian Splatting within a physically based, relightable model tailored for the complex acquisition conditions in endoscopy, such as restricted camera rotations and strong view-dependent illumination. By exploiting the connection between the camera and light source, our approach introduces a relighting model to capture the intricate interactions between light and tissue using physically based rendering and MLP. Existing methods often produce artifacts and inconsistencies under these conditions, which PR-ENDO overcomes by incorporating a specialized diffuse MLP that utilizes light angles and normal vectors, achieving stable reconstructions even with limited training camera rotations. We benchmarked our framework using a publicly available dataset and a newly introduced dataset with wider camera rotations. Our methods demonstrated superior image quality compared to baseline approaches

**DnFPlane for Efficient and High-Quality 4D Reconstruction of Deformable Tissues**
https://link.springer.com/chapter/10.1007/978-3-031-72089-5_17
Reconstruction of deformable tissues in robotic surgery from endoscopic stereo videos holds great significance for a variety of clinical applications. Existing methods primarily focus on enhancing inference speed, overlooking depth distortion issues in reconstruction results, particularly in regions occluded by surgical instruments. This may lead to misdiagnosis and surgical misguidance. In this paper, we propose an efficient algorithm designed to address the reconstruction challenges arising from depth distortion in complex scenarios. Unlike previous methods that treat each feature plane equally in the dynamic and static field, our framework guides the static field with the dynamic field, generating a dynamic-mask to filter features at the time level. This allows the network to focus on more active dynamic features, reducing depth distortion. In addition, we design a module to address dynamic blurring. Using the dynamic-mask as a guidance, we iteratively refine color values through Gated Recurrent Units (GRU), improving the clarity of tissues detail in the reconstructed results. Experiments on a public endoscope dataset demonstrate that our method outperforms existing state-of-the-art methods without compromising training time. Furthermore, our approach shows outstanding reconstruction performance in occluded regions, making it a more reliable solution in medical scenarios. Code is available: https://github.com/CUMT-IRSI/DnFPlane.git.

**EndoSelf: Self-supervised Monocular 3D Scene Reconstruction of Deformable Tissues with Neural Radiance Fields on Endoscopic Videos**
https://link.springer.com/chapter/10.1007/978-3-031-72089-5_23
Neural radiance fields have recently emerged as a powerful representation to reconstruct deformable tissues from endoscopic videos. Previous methods mainly focus on depth-supervised approaches based on endoscopic datasets. As additional information, depth values were proven important in reconstructing deformable tissues by previous methods. However, collecting a large number of datasets with accurate depth values limits the applicability of these approaches for endoscopic scenes. To address this issue, we propose a novel self-supervised monocular 3D scene reconstruction method based on neural radiance fields without prior depth as supervision. We consider the monocular 3D reconstruction based on two approaches: ray-tracing-based neural radiance fields and structure-from-motion-based photogrammetry. We introduce structure from motion framework and leverage color values as a supervision to complete the self-supervised learning strategy. In addition, we predict the depth values from neural radiance fields and enforce the geometric constraint for depth values from adjacent views. Moreover, we propose a looped loss function to fully explore the temporal correlation between input images. The experimental results showed that the proposed method without prior depth outperformed the previous depth-supervised methods on two endoscopic datasets. Our code is available at https://github.com/MoriLabNU/EndoSelf.

**Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting**
https://link.springer.com/chapter/10.1007/978-3-031-72089-5_19
In the realm of robot-assisted minimally invasive surgery, dynamic scene reconstruction can significantly enhance downstream tasks and improve surgical outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to prominence for their exceptional ability to reconstruct scenes but are hampered by slow inference speed, prolonged training, and inconsistent depth estimation. Some previous work utilizes ground truth depth for optimization but it is hard to acquire in the surgical domain. To overcome these obstacles, we present Endo-4DGS, a real-time endoscopic dynamic reconstruction approach that utilizes 3D Gaussian Splatting (GS) for 3D representation. Specifically, we propose lightweight MLPs to capture temporal dynamics with Gaussian deformation fields. To obtain a satisfactory Gaussian Initialization, we exploit a powerful depth estimation foundation model, Depth-Anything, to generate pseudo-depth maps as a geometry prior. We additionally propose confidence-guided learning to tackle the ill-pose problems in monocular depth estimation and enhance the depth-guided reconstruction with surface normal constraints and depth regularization. Our approach has been validated on two surgical datasets, where it can effectively render in real-time, compute efficiently, and reconstruct with remarkable accuracy. Our code is available at https://github.com/lastbasket/Endo-4DGS.

**Diffuser endoscopy for single-shot fluorescence imaging using physics-informed deep learning**
https://www.spiedigitallibrary.org/conference-proceedings-of-spie/PC13118/PC1311807/Diffuser-endoscopy-for-single-shot-fluorescence-imaging-using-physics-informed/10.1117/12.3026717.short

Minimally invasive endoscopy using multicore fibers shows great potential for numerous applications in biomedical imaging. With a diffuser on the distal side of the fiber and image recovery using AI, single-shot 3D imaging is possible by encoding the image volume into 2D speckle patterns. In comparison to equivalent lens systems, a higher space-bandwidth product can be achieved. However, decoding the image with iterative algorithms is time-consuming. Thus, we propose utilizing a neural network for fast 2D and 3D image reconstruction at video rate. In this work, single-shot 3D fluorescence imaging with physics-informed neural network is presented, which is promising for calcium imaging at in vivo brain diagnostics.

**EndoPerfect: A Hybrid NeRF-Stereo Vision Approach Pioneering Monocular Depth Estimation and 3D Reconstruction in Endoscopy**
https://arxiv.org/abs/2410.04041
3D reconstruction in endoscopic sinus surgery (ESS) demands exceptional accuracy, with the mean error and standard deviation necessitating within the range of a single CT slice (0.625 mm), as the critical structures in the nasal cavity are situated within submillimeter distances from surgical instruments. This poses a formidable challenge when using conventional monocular endoscopes. Depth estimation is crucial for 3D reconstruction, yet existing depth estimation methodologies either suffer from inherent accuracy limitations or, in the case of learning-based approaches, perform poorly when applied to ESS despite succeeding on their original datasets. In this study, we present a novel, highly generalizable method that combines Neural Radiance Fields (NeRF) and stereo depth estimation for 3D reconstruction that can derive metric monocular depth. Our approach begins with an initial NeRF reconstruction yielding a coarse 3D scene, the subsequent creation of binocular pairs within coarse 3D scene, and generation of depth maps through stereo vision, These depth maps are used to supervise subsequent NeRF iteration, progressively refining NeRF and binocular depth, the refinement process continues until the depth maps converged. This recursive process generates high-accuracy depth maps from monocular endoscopic video. Evaluation in synthetic endoscopy shows a depth accuracy of 0.125 ± 0.443 mm, well within the 0.625 mm threshold. Further clinical experiments with real endoscopic data demonstrate a mean distance to CT mesh of 0.269 mm, representing the highest accuracy among monocular 3D reconstruction methods in ESS.

**Disparity estimation of stereo-endoscopic images using deep generative network**
https://www.sciencedirect.com/science/article/pii/S2405959524001206
A novel disparity estimation pipeline is proposed for 3D reconstruction of dynamic soft tissues in minimally invasive surgery (MIS), which uses a deep generative network to learn manifold distributions of reasonable disparity maps from past stereo images in the training phase, and transforms stereo matching into an optimization problem with respect to the low-dimensional latent vector of the learned generator in the application phase. The proposed pipeline is particularly suitable for dynamic MIS scenarios with insufficient training data, as the photometric loss is explicitly used in the application phase and the scenario priors are introduced via a deep generative network.

**Analysis of modeling nasal narrow space based on visual slam**
https://www.ewadirect.com/proceedings/tns/article/view/16398
Abstract. Now, surgeries are becoming more stable, safe, efficient and low-cost. During the surgical treatment of nasal diseases, surgical robotic robotics can help operate accurately and reduce the discomfort after surgery. However, due to the internal space of the nasal cavity being relatively narrow, it is difficult for the nasal surgical robot to contain multiple vision sensors and the monocular camera could not get information about the depth of the 3D objects in the scene, so the existing surgical robots cannot accomplish the three-dimensional modeling about the internal space of nasal cavity well. In practice, doctors still have to analyze pictures from the robotic, which may decrease the efficiency of the surgery and increase the risk to patients. This article designed a SLAM algorithm framework based on a depth estimation network, it can simulate the internal structure of the nasal cavity more accurately through pictures, which come from monocular endoscopic on surgical robotic. The insights gained in this study verify that the method of image segmentation can also make the depth representation of the nasal internal space more accurate and this method may help robots realize their self-position in the narrow area of the nasal cavity, which lays the foundations for the development of fully autonomous surgical robots.

**SurgicalGS: Dynamic 3D Gaussian Splatting for Accurate Robotic-Assisted Surgical Scene Reconstruction**
https://arxiv.org/abs/2410.09292
Accurate 3D reconstruction of dynamic surgical scenes from endoscopic video is essential for robotic-assisted surgery. While recent 3D Gaussian Splatting methods have shown promise in achieving high-quality reconstructions with fast rendering speeds, their use of inverse depth loss functions compresses depth variations. This can lead to a loss of fine geometric details, limiting their ability to capture precise 3D geometry and effectiveness in intraoperative application. To address these challenges, we present SurgicalGS, a dynamic 3D Gaussian Splatting framework specifically designed for surgical scene reconstruction with improved geometric accuracy. Our approach first initialises a Gaussian point cloud using depth priors, employing binary motion masks to identify pixels with significant depth variations and fusing point clouds from depth maps across frames for initialisation. We use the Flexible Deformation Model to represent dynamic scene and introduce a normalised depth regularisation loss along with an unsupervised depth smoothness constraint to ensure more accurate geometric reconstruction. Extensive experiments on two real surgical datasets demonstrate that SurgicalGS achieves state-of-the-art reconstruction quality, especially in terms of accurate geometry, advancing the usability of 3D Gaussian Splatting in robotic-assisted surgery.

**DESR: dynamic endoscopic scene reconstruction based on Gaussian splatting**
https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13399/133990A/DESR-dynamic-endoscopic-scene-reconstruction-based-on-Gaussian-splatting/10.1117/12.3053129.short
Reconstructing soft tissues in endoscopic videos requires the support of advanced 3D reconstruction technology, which plays a key role in robot-assisted minimally invasive surgery. In this paper, we focus on how to reconstruct dynamic soft tissues as realistically as possible with surgical instruments removed. Due to the limited viewing angle and complex soft tissue deformation, current methods suffer from reconstruction blur and long training time. Inspired by the powerful rendering ability of Gaussian Splatting, we propose a dynamic endoscopic scene reconstruction method which yields better rendering quality and real-time rendering speed. We first utilize depth information to project image to point cloud and give 3D Gaussian points initial positions and colors. The properties of Gaussian points at a certain time stamp are predicted by a MLP deformation module. Our network then renders 3D Gaussian points to get 2D images using a tile-based rasterizer. We design reconstruction losses and color variation loss to optimize the rendering results. Extensive experiments show that our network generates high-fidelity reconstructed endoscopic videos with real-time rendering speed.

**A Review of 3D Reconstruction Techniques for Deformable Tissues in Robotic Surgery**
https://arxiv.org/abs/2408.04426
As a crucial and intricate task in robotic minimally invasive surgery, reconstructing surgical scenes using stereo or monocular endoscopic video holds immense potential for clinical applications. NeRF-based techniques have recently garnered attention for the ability to reconstruct scenes implicitly. On the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly using 3D Gaussians and projects them onto a 2D plane as a replacement for the complex volume rendering in NeRF. However, these methods face challenges regarding surgical scene reconstruction, such as slow inference, dynamic scenes, and surgical tool occlusion. This work explores and reviews state-of-the-art (SOTA) approaches, discussing their innovations and implementation principles. Furthermore, we replicate the models and conduct testing and evaluation on two datasets. The test results demonstrate that with advancements in these techniques, achieving real-time, high-quality reconstructions becomes feasible.

**CAPTIV8: A Comprehensive Large Scale Capsule Endoscopy Dataset For Integrated Diagnosis**
https://ieeexplore.ieee.org/abstract/document/10647633
Limited access to high-quality medical data poses a significant obstacle to automated diagnoses in medical modalities like Wireless Capsule Endoscopy (WCE), hindering potential advancements in automated medical diagnoses. This study presents a meticulously curated WCE dataset CAPTIV8, focused on the large colon and its pathologies, including Ulcerative Colitis (UC). Comprising a total of 1352 short video segments, totaling more than 200,000 frames with high mucosal visibility, the dataset features eight distinct types of pathology, along with signs of UC, accompanied by clinician-assigned text descriptions. To enhance its medical utility, the dataset integrates overlapping diagnoses from three diagnostic modalities: traditional and capsule endoscopy, and histology. Key attributes such as cleansing scores, text reports, capsule camera calibration and localization data have been incorporated to broaden its applicability in medical and artificial intelligence research. Designed for a wide spectrum of research challenges, from basic classification tasks to 3D reconstruction, CAPTIV8 aims to advance the incorporation of automated solutions in WCE diagnosis. The dataset can be accessed here:https:// dataverse.no/dataset.xhtml?persistentId=doi:10.18710/BSXNA

**Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy**
https://arxiv.org/abs/2409.07723
Depth estimation is a cornerstone of 3D reconstruction and plays a vital role in minimally invasive endoscopic surgeries. However, most current depth estimation networks rely on traditional convolutional neural networks, which are limited in their ability to capture global information. Foundation models offer a promising avenue for enhancing depth estimation, but those currently available are primarily trained on natural images, leading to suboptimal performance when applied to endoscopic images. In this work, we introduce a novel fine-tuning strategy for the Depth Anything Model and integrate it with an intrinsic-based unsupervised monocular depth estimation framework. Our approach includes a low-rank adaptation technique based on random vectors, which improves the model's adaptability to different scales. Additionally, we propose a residual block built on depthwise separable convolution to compensate for the transformer's limited ability to capture high-frequency details, such as edges and textures. Our experimental results on the SCARED dataset show that our method achieves state-of-the-art performance while minimizing the number of trainable parameters. Applying this method in minimally invasive endoscopic surgery could significantly enhance both the precision and safety of these procedures.

**Online 3D Reconstruction and Dense Tracking in Endoscopic Videos**
https://link.springer.com/chapter/10.1007/978-3-031-72089-5_42
3D scene reconstruction from stereo endoscopic video data is crucial for advancing surgical interventions. In this work, we present an online framework for online, dense 3D scene reconstruction and tracking, aimed at enhancing surgical scene understanding and assisting interventions. Our method dynamically extends a canonical scene representation using Gaussian splatting, while modeling tissue deformations through a sparse set of control points. We introduce an efficient online fitting algorithm that optimizes the scene parameters, enabling consistent tracking and accurate reconstruction. Through experiments on the StereoMIS dataset, we demonstrate the effectiveness of our approach, outperforming state-of-the-art tracking methods and achieving comparable performance to offline reconstruction techniques. Our work enables various downstream applications thus contributing to advancing the capabilities of surgical assistance systems.

**Three-dimensional endoscopic imaging system based on micro-lithography mask structured light projection**
https://www.sciencedirect.com/science/article/pii/S0030401824010320
To achieve effective in-situ endoscopic diagnosis and treatment, the measurement of the size of lesions (such as tumors) and the characterization of their shape are important. However, the application of binocular endoscopy is still limited due to issues such as the lack of texture in some scenes, difficulty in matching, and large computational load. To address this, we have developed a 3D endoscopic imaging system based on micro-lithography mask structured light projection to measure the shape and size of targets within the endoscopic view. Firstly, a brand new mechanical design was implemented for the endoscope tip to integrate both white light and structured light channels. Then, a projection lens based on Q-type aspheric design and a micro-lithography mask based on the M-array were designed to achieve high contrast and high-resolution structured light projection in the endoscopic scene. Finally, by identifying feature points in the target and reference images, pixel matching and disparity calculation were achieved, allowing for 3D reconstruction. Our proposed 3D endoscopic imaging system was validated in a gastric model and a cervical model, where the model was reconstructed and compared with the ground truth, yielding mean RMSE of 0.20–0.31 mm at a working distance of about 40 mm, thus confirming the effectiveness of our system.

**UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views**
https://ieeexplore.ieee.org/abstract/document/10750866
Visualizing surgical scenes is crucial for revealing internal anatomical structures during minimally invasive procedures. Novel View Synthesis is a vital technique that offers geometry and appearance reconstruction, enhancing understanding, planning, and decision-making in surgical scenes. Despite the impressive achievements of Neural Radiance Field (NeRF), its direct application to surgical scenes produces unsatisfying results due to two challenges: endoscopic sparse views and significant photometric inconsistencies. In this paper, we propose uncertainty-aware conditional NeRF for novel view synthesis to tackle the severe shape-radiance ambiguity from sparse surgical views. The core of UC-NeRF is to incorporate the multi-view uncertainty estimation to condition the neural radiance field for modeling the severe photometric inconsistencies adaptively. Specifically, our UC-NeRF first builds a consistency learner in the form of multi-view stereo network, to establish the geometric correspondence from sparse views and generate uncertainty estimation and feature priors. In neural rendering, we design a base-adaptive NeRF network to exploit the uncertainty estimation for explicitly handling the photometric inconsistencies. Furthermore, an uncertainty-guided geometry distillation is employed to enhance geometry learning. Experiments on the SCARED and Hamlyn datasets demonstrate our superior performance in rendering appearance and geometry, consistently outperforming the current state-of-the-art approaches. Our code will be released at https://github.com/wrld/UC-NeRF.

**DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model**
https://arxiv.org/abs/2408.17433
Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D reconstruction and visualization. While foundation models like Depth Anything Models (DAM) show promise, directly applying them to surgery often yields suboptimal results. Fully fine-tuning on limited surgical data can cause overfitting and catastrophic forgetting, compromising model robustness and generalization. Although Low-Rank Adaptation (LoRA) addresses some adaptation issues, its uniform parameter distribution neglects the inherent feature hierarchy, where earlier layers, learning more general features, require more parameters than later ones. To tackle this issue, we introduce Depth Anything in Robotic Endoscopic Surgery (DARES), a novel approach that employs a new adaptation technique, Vector Low-Rank Adaptation (Vector-LoRA) on the DAM V2 to perform self-supervised monocular depth estimation in RAS scenes. To enhance learning efficiency, we introduce Vector-LoRA by integrating more parameters in earlier layers and gradually decreasing parameters in later layers. We also design a reprojection loss based on the multi-scale SSIM error to enhance depth perception by better tailoring the foundation model to the specific requirements of the surgical environment. The proposed method is validated on the SCARED dataset and demonstrates superior performance over recent state-of-the-art self-supervised monocular depth estimation techniques, achieving an improvement of 13.3% in the absolute relative error metric. The code and pre-trained weights are available at this https URL.

**Endoscopic Stereo Vision for Robotic 3D Detection of Thin Wire Features**
https://ieeexplore.ieee.org/abstract/document/10711442
Vision systems are more and more employed in robotic applications to perceive the surrounding environment and to make decisions accordingly. Despite this, the 3D reconstruction is a non-trivial problem for thin deformable linear objects. The cameras on the market that allow the 3D modelling of very small objects are bulky and expensive, and as a consequence cannot be easily integrated with a robotic system. This paper proposes a stereo vision system completely integrable into the robot end-effector, composed of two low-cost off-the-shelf endoscopic cameras with the aim of detecting tiny wires in the scene and identifying their location and diameter. The proposed method splices state-of-the-art vision algorithms applied to macro cameras obtaining, for wires with a diameter of few millimeters or less than a millimeter, a diameter estimation error lower than 10% and a location estimation error lower than 3%. The presented approach can be generalized to different types of endoscopic cameras and thin target objects.

**Enhanced Scale-Aware Depth Estimation for Monocular Endoscopic Scenes with Geometric Modeling**
https://link.springer.com/chapter/10.1007/978-3-031-72089-5_25
Scale-aware monocular depth estimation poses a significant challenge in computer-aided endoscopic navigation. However, existing depth estimation methods that do not consider the geometric priors struggle to learn the absolute scale from training with monocular endoscopic sequences. Additionally, conventional methods face difficulties in accurately estimating details on tissue and instruments boundaries. In this paper, we tackle these problems by proposing a novel enhanced scale-aware framework that only uses monocular images with geometric modeling for depth estimation. Specifically, we first propose a multi-resolution depth fusion strategy to enhance the quality of monocular depth estimation. To recover the precise scale between relative depth and real-world values, we further calculate the 3D poses of instruments in the endoscopic scenes by algebraic geometry based on the image-only geometric primitives (i.e., boundaries and tip of instruments). Afterwards, the 3D poses of surgical instruments enable the scale recovery of relative depth maps. By coupling scale factors and relative depth estimation, the scale-aware depth of the monocular endoscopic scenes can be estimated. We evaluate the pipeline on in-house endoscopic surgery videos and simulated data. The results demonstrate that our method can learn the absolute scale with geometric modeling and accurately estimate scale-aware depth for monocular scenes. Code is available at: https://github.com/med-air/MonoEndoDepth.

**Triangular Prism-based Omnidirectional Imaging System**
https://ieeexplore.ieee.org/abstract/document/10658729
Omnidirectional images have a significantly broader field of view (FOV) compared to standard cameras. Various methods have been used to achieve an omnidirectional view. Ideally, this can be achieved by merging multiple images taken by one or more cameras into a single merged image. An application example of this is an endoscope, which is used for visualizing the internal structures of the human body. However, despite the numerous techniques and methods available for achieving an integrated 360-degree view, there are limitations in processing this in real-time due to the number of cameras and data size. In this paper, we present a novel vision system tip design to reduce the processing time by using fewer cameras compared to the current methods in the literature.

**xLSTM-UNet can be an Effective Backbone for 2D & 3D Biomedical Image Segmentation Better than its Mamba Counterparts**
https://openreview.net/forum?id=YG5lOzKJOr#discussion
Convolutional Neural Networks (CNNs) and Vision Transformers (ViT) have been pivotal in biomedical image segmentation. Yet, their ability to manage long-range dependencies remains constrained by inherent locality and computational overhead. To overcome these challenges, in this technical report, we first propose xLSTM-UNet, a UNet structured deep learning neural network that leverages Vision-LSTM (xLSTM) as its backbone for medical image segmentation. xLSTM has recently been proposed as the successor of Long Short-Term Memory (LSTM) networks and has demonstrated superior performance compared to Transformers and State Space Models (SSMs) like Mamba in Neural Language Processing (NLP) and image classification (as demonstrated in Vision-LSTM, or ViL implementation). Here, we provide the first integration of xLSTM with image segmentation backbone -- namely xLSTM-U, which extend the success of xLSTM in the biomedical image segmentation domain. By integrating the local feature extraction strengths of convolutional layers with the long-range dependency-capturing abilities of xLSTM, the proposed xLSTM-UNet offers a robust solution for comprehensive image analysis. We validate the efficacy of xLSTM-UNet through experiments. Our findings demonstrate that xLSTM-UNet consistently surpasses the performance of leading CNN-based, Transformer-based, and Mamba-based segmentation networks in multiple datasets in biomedical segmentation including organs in abdomen MRI, instruments in endoscopic images, and cells in microscopic images. With comprehensive experiments performed, this paper highlights the potential of xLSTM-based architectures in advancing biomedical image analysis in both 2D and 3D. We believe this new finding will be of interest to the research community and may inspire future studies. The code, models, and datasets are publicly available at https://github.com/tianrun-chen/xLSTM-UNet-PyTorch/tree/main.
